{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling and split-apply-combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big data is only useful if we know how to wrangle it. `pandas` helps us do that with a number of nifty functions, and `seaborn` often has great plotting functions to visualize the results. \n",
    "\n",
    "Please read the following:\n",
    "1. Read about the principle of [split-apply-combine](https://pandas.pydata.org/pandas-docs/dev/user_guide/groupby.html)\n",
    "2. Peruse the pandas [cookbook](https://pandas.pydata.org/pandas-docs/dev/user_guide/cookbook.html) for clever ways to slice, group, and query data. \n",
    "3. Read the introduction to the [`seaborn`](https://seaborn.pydata.org/tutorial/introduction) package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced `pandas` operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to load in a very large dataset from my Arctic drainage density work. This is every watershed I analyzed for drainage density (the 'DD' column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheds = pd.read_csv('https://raw.githubusercontent.com/jmdelvecchio/arctic-drainage-density/main/watershed_export.csv')\n",
    "\n",
    "sheds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column values are:\n",
    "- `Unnamed: 0` is the index of the larger dataframe that I cleaned up. Ignore (and I should have dropped before exporting! Oops)\n",
    "- `long` is longitude of the center of the watershed\n",
    "- `lat` is latitude of the center of the watershed\n",
    "- `HYBAS_ID` is the code associated with the associated [Level 10 watershed in the HydrpSHEDS dataset](https://developers.google.com/earth-engine/datasets/catalog/WWF_HydroSHEDS_v1_Basins_hybas_10). Each watershed in the dataset has a unique code. \n",
    "- `segment_ct` is the number of pixels of stream segments used to calculate drainage density\n",
    "- `flow_acc` is the total number of pixels in the flow accumulation raster. `segment_ct` divided by `flow_acc` gives the drainage density `DD`.\n",
    "- `relief` is the maximum elevation minus the minimum elevation within the watershed boundary\n",
    "- `map_mean' is the *mean* mean annual precipiation value (in mm) within the watershed boundary\n",
    "- `mat_mean` is the *mean* mean annual temperature value (in C) within the watershed boundary\n",
    "- `ndvi_mean` is the *mean* of the *maximum* annual [normalized difference vegetation index](https://modis.gsfc.nasa.gov/data/dataprod/mod13.php) within the watershed boundary\n",
    "- `glaciated` is the category of whether the watershed was in glacial boundaries during the Last Glacial Maximum, Marine Isotope Stage 6, or Not Glaciated\n",
    "- `EXTENT` is the permafrost extent as defined by this [circum-Arctic map](https://nsidc.org/data/ggd318/versions/2)\n",
    "- `DD` is drainage density, as defined above\n",
    "\n",
    "*(if you want to know more you can always [read the paper](https://eartharxiv.org/repository/view/5340/)!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice using Google and reading documentation to tackle these data wrangling tasks of increasing difficulty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many watersheds of each category of permafrost extent are in this DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the *mean* mean annual temperature of watersheds that are **unglaciated** and in **continuous permafrost**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a **list** of HYBAS IDs for a **random sample** of unglaciated watersheds in continuous permafrost whose relief is greater than 50 meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seaborn` has excellent built-in functions to visualize multifaceted data stored in a DataFrame. In particular the `hue` keyword in most plots allows you to group categorial data together when plotting otherwise continuous data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extent_order = ['Continuous', 'Discontinuous', 'Sporadic', 'Isolated', 'No permafrost']\n",
    "sns.barplot(data=sheds, x='EXTENT', y='mat_mean', hue='glaciated', order=extent_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicate the boxplot from my paper's supplement that shows how drainage density varies as a function of both permafrost extent and glacial history. Check out the seaborn boxplot documentation to customize the plot to be as close to mine as possible (not worrying about the statistical annotations). Bonus points if you can replicate the plot in the main figure that does some collapsing of the extent categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `geopandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables to maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataframe with latitude and longitude data can be convered into a dataframe if we specify the geometry as the appropriate columns and the appropriate [coordinate reference system](https://geopandas.org/en/latest/docs/reference/api/geopandas.GeoDataFrame.crs.html) (lat/long data will usually be WGS84, EPSG:4326). We can use the `gpd.points_from_xy()` method to create a geodataframe from a regular old dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheds_gdf = gpd.GeoDataFrame(\n",
    "    sheds, # what dataframe are we using?\n",
    "    geometry=gpd.points_from_xy(\n",
    "        sheds['long'], sheds['lat']), # here we specify which columns have the long and lat\n",
    "          crs='epsg:4326' # finally end specifying the WGS84 spatial reference\n",
    "          ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the drainage density results on a world map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just grab world boundaries\n",
    "world = gpd.read_file('ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5),dpi=300)\n",
    "im = world.boundary.plot(\n",
    "    color='k', edgecolor='black', zorder=0, ax=ax)\n",
    "\n",
    "result = sheds_gdf.plot(\n",
    "        ax=ax,\n",
    "        column='DD',\n",
    "        vmin=0.08,\n",
    "        vmax=.16,\n",
    "        cmap='seismic',\n",
    "        s=0.25, #size of point\n",
    "        legend=True,\n",
    "        legend_kwds={\n",
    "            'label': \"what's a good colorbar label?\",\n",
    "            'orientation': \"horizontal\"\n",
    "            },\n",
    "            zorder=1\n",
    "        )\n",
    "\n",
    "ax.set_ylim(20,90)\n",
    "\n",
    "ax.set_ylabel(\"ylabel\")\n",
    "ax.set_xlabel(\"xlabel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another great way to explore data is `gpd.explore()` (but it opens an interactive window, so large datasets might fail to load or load slowly, beware!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sheds_gdf.explore(column='DD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in `CALM_data_regression.csv`. A previous student and I took some [fairly messy data](https://www2.gwu.edu/~calm/data/north.htm) on field observations of active layer thickness in permafrost, cleaned it up, and performed a regression on each year's measurements to get the **trend in active-layer thickness** (given as the `slope` column). Turn these data, which are points that have latitudes and longitudes, into a map colored by the **trend in active layer thickness** (with appropriate unit and axis labels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# To make the next part easier make sure your GeoDataFrame is named `calm_gdf`\n",
    "calm_gdf = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to be able to compare two geospatial datasets. For example, it would be cool to know which of our watersheds might be close enough to the CALM sites that we can talk about how they've changed over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chained functions allow me to convert the CALM data to something that has units in meters\n",
    "# And then create a 10 km buffer around the site\n",
    "# And then put it back to lat/long\n",
    "calm_gdf_buffer_geom = calm_gdf.to_crs('EPSG:3413').geometry.buffer(10000).to_crs('EPSG:4326')\n",
    "\n",
    "# Create a new GeoDataFrame whose geometry is the buffer rather than the point. \n",
    "calm_buffer = gpd.GeoDataFrame(calm_gdf.copy(), geometry=calm_gdf_buffer_geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the [sjoin](https://geopandas.org/en/stable/docs/reference/api/geopandas.sjoin.html) function to spatially intersect our CALM site buffer against the watershed centroid points to find **watersheds that are close to CALM sites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = gpd.sjoin(calm_buffer, sheds_gdf, how=\"inner\")\n",
    "\n",
    "join.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many **CALM** sites are near *more than one watershed*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a **list** of HYBAS IDs of **watersheds near CALM sites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `pathlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have written a lot of code to navigate directories with `pathlib`. You can read about `pathlib`[here](https://docs.python.org/3/library/pathlib.html) in the official documentaton or another guide [here](https://realpython.com/python-pathlib/). One of the strategies I have for working with my big datasets is that I often **store files related to a watershed in a directory named with the HYBAS ID** so I can iterate through directories as I might through a list from a DataFrame. \n",
    "\n",
    "For now we are just going to focus on making new directories and saving files into them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pathlib, paths are represented as objects. You can create a Path object by passing a string representing the path to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Path object for the current directory\n",
    "current_directory = Path()\n",
    "\n",
    "# Creating a Path object for a specific directory\n",
    "specific_directory = Path(\"/path/to/your/directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to quickly make a new directory and then save a file into that directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create two new directories\n",
    "# The slash is an easy way to join an existing directory to a string you want to create into a new directory\n",
    "directory1 = current_directory / \"calm_sheds_intersections\"\n",
    "\n",
    "directory1.mkdir(exist_ok=True)  # exist_ok=True prevents an error if the directory already exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `pd.to_csv()`, (1) save a `pandas` dataframe of the resulting spatial join and then (2) open the dataframe from its new location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bog_ms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
